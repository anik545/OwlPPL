% !TeX root = ../diss.tex

% sketch out what plots to include here
% - plot of kl divergence w.r.t. number of samples taken
% - plot of running time w.r.t. amount of data conditioned on
% - plot of running time w.r.t. number of particles (for smc)
% - maximum memory footprint (against parameters as above)
% - total memory footprint over time
% box plots instead of just lines

% use ppl to do evaluation? good to show code in my language which does evaluation

% explain WHY in evaluation, how just what - e.g. explain why plot looks how it does, don't just describe

% show some output from merlin to show the fact that type inference works and is good?
\chapter{Evaluation}
\vspace{1cm}
\textit{So far, I have developed a PPL that can be used to define arbitrary probabilistic models and perform Bayesian inference on them. To evaluate the performance of my PPL, I will present some examples to show programs written in my PPL, translated into equivalent programs in other PPLs, and then measure time and memory consumption of inference\footnote{All tests are carried out on a single core of an Intel\textsuperscript{(R)} Core\textsuperscript{(TM)} i5-7200U CPU @ 2.50GHz}. I will also determine the correctness of inference procedures on simple problems by using hypothesis tests to assert posterior samples fit the expected distribution.}

\section{Examples}
To show how my PPL would be used on real problems, I now outline a set of example problems. The first examples here are simple, and have analytic solutions - this is so that I can then test the correctness of applying inference on them. More complex realistic models are also included to test performance. Full derivations of the solutions as well as mathematical descriptions of the models are given in appendix \ref{app:examples}.

\subsection{Sprinkler}
% to show exact inference on discrete model
The sprinkler model is a commonly used example in Bayesian inference due to it's simplicity. It is an example of a \textit{Bayesian network}, and can be visualised as in figure \ref{fig:sprinkler-network}. The code in listing \ref{lst:sprinkler} shows the model in the diagram encoded as a program. This particular program models the probability of rain given that the grass is wet.

% \begin{noindent}
\begin{figure*}[!htb]
	\centering
	\begin{minipage}{0.47\linewidth}
		\ocamlcode{code_snippets/sprinkler.ml}	
		\captionof{listing}{Sprinkler model in OwlPPL}
		\label{lst:sprinkler}
	\end{minipage}
	\begin{minipage}{0.47\linewidth}
		\includegraphics[width=\linewidth]{figs/sprinkler-network.png}
		\captionof{figure}{Sprinkler model as a network}
		\label{fig:sprinkler-network}
		\vspace{0.3cm}
		\begin{ocamlcode-in}
let () =
  exact_inference sprinkler_model
  |> print_exact_bool
(*false: 0.13706 true: 0.86294*)
		\end{ocamlcode-in}
		\captionof{listing}{Output of inference}
		\label{lst:inf-output}
	\end{minipage}
	% \caption{}
	% \label{}
\end{figure*}
% \end{noindent}

\subsection{Biased Coin} \label{sec:coin} 
Modelling a biased coin shows an example of a very simple model with a continuous posterior that can be calculated analytically \cite{datasci}. The problem is that given a coin, we flip it 10 times and observe 9 heads. We not want to find out whether or not the coin is biased and with what weight (how likely is it to flip a heads). This model uses an uninformative prior, and the posterior over the weight of the coin is $\text{Beta}(10,2)$ (derivation given in \ref{app:coin})

The program in my PPL is shown in listing \ref{lst:coin}, and demonstrates setting up the model, performing inference as well as finding the mean of the posterior. The application is to find the chance of the next coin flip landing heads.

\begin{listing}[!ht]
	\ocamlcode{code_snippets/coin.ml}	
	\caption{Coin model - getting the mean of the posterior}
	\label{lst:coin}
\end{listing}


\begin{figure}[!htb]
	\begin{minipage}{0.5\textwidth}
		\centering
		\jscode{code_snippets/webppl/coin.js}
		\captionof{listing}{WebPPL}
	\end{minipage}
	\begin{minipage}{0.5\textwidth}
		\centering
		\clojurecode{code_snippets/anglican/coin.clj}
		\captionof{listing}{Anglican}
	\end{minipage}
	\caption{The coin model in WebPPL (JS) and Anglican (Clojure)}
	\label{fig:compare-coin}
\end{figure}

The comparison given in Figure \ref{fig:compare-coin} shows how the same model is defined in other languages. Both languages use similar constructs, despite differing syntax. This example also shows that my PPL is similar to existing systems, and is not more verbose.

\subsection{HMM}
Hidden Markov models are slightly more involved models, where we have a sequence of hidden states, which emit observed states. There are two distributions involved here, the transition distribution, which defines how likely the next state is given the current state, and the emission distribution, which is the distribution over the observed states given the hidden state. The example in Listing \ref{lst:hmm} uses discrete distributions, but any type of distribution can be used. The exact posterior for simple models can be found using the forward-backward algorithm.
% use forward-backward to get exact posterior
\begin{listing}[!ht]
	\ocamlcode{code_snippets/hmm.ml}
	\caption{Hidden Markov Model}
	\label{lst:hmm}
\end{listing}

\subsection{Linear Regression}
This example shows how to use multiple data points to infer a continuous distribution. This example can be used to infer the parameters of a line through a set of 2-D points. The fold function can be used to condition on many observations easily. The fmap function is used to map outputs from a distribution. Since the linear regression model produces tuples of parameters, we can create individual distributions over either one.

\begin{listing}[!ht]
	\ocamlcode{code_snippets/linreg.ml}	
	\caption{Linear Regression}
	\label{lst:linreg}
\end{listing}

\subsection{Infinite Mixture Model - Dirichlet Process}
This example demonstrates the common task of clustering a set of data points without knowledge of the number of clusters. This is a model which cannot be expressed in PPLs such as STAN or Infer.Net, since it is a non-parametric Bayesian model - we do not know the number of clusters beforehand. The infinite nature of this model requires the use of unbounded recursion which terminates with probability 1.

The prior for this model is a Dirichlet process \cite{teh2010dirichlet}, which defines a distribution over distributions. This model is then known as a Dirichlet Process mixture model with an infinite number of Gaussians components \cite{dpmm}, and the number of clusters is allowed to grow with the dataset size. I use a mixture of Gaussians, meaning the likelihood of a point belonging to each cluster is given by different normal distributions. The full code for this model, along with comparisons to other languages is given in appendix \ref{app:dp}.


% Need to show examples which can't be done in graph based thing - maybe geometric somewhere
% Need to explain why these examples are actually difficult.
% http://www.cs.cmu.edu/~epxing/Class/10708-16/slide/lecture18-DP.pdf


\section{Statistical tests}
To evaluate the correctness of my PPL, I used statistical tests which measure goodness-of-fit, i.e. how similar two distributions are to each other. I compare the empirical distribution of 10,000 samples from an approximated distribution to an exact distribution which is calculated analytically. Test distributions (e.g. the $\chi^2$ distribution) were calculated using \texttt{Owl}, and functions to calculate the test statistics and p-values are given in the \texttt{Evaluation} module.

For all tests described below, I set the significance level $\alpha = 0.05$ and use null and alternative hypotheses as follows:

$H_0:$ The sample data follow the desired distribution\\
$H_1:$ The sample data do not follow the desired distribution

\subsection{Chi-squared}

The $\chi^2$ test is a simple goodness-of-fit test which can test discrete distributions. The test statistic is as follows, with each $i$ being a distinct element in the distribution, $x_i$ is the observed number of samples with the value $i$, and $m_i$ is the expected number of samples for the value $i$.
% 
\[X^{2}=\sum _{i=1}^{k}{\frac {(x_{i}-m_{i})^{2}}{m_{i}}}\]
% 
This test statistic is compared against the critical value (at the significance level) of the chi-squared distribution, with $k-1$ degrees of freedom , where k is the number of possible values of the distribution.

\subsubsection{Results}
\begin{table}[!ht]
	\centering
	% \csvautotabular{data/hypothesis/hypothesis-chi.csv}
	\pgfplotstableread[col sep=comma,]{data/hypothesis/hypothesis-chi.csv}\normal
	\pgfplotstabletranspose[colnames from = 0]\transpose\normal
	\pgfplotstabletypeset[
		every row/.style={/pgf/number format/sci},
		every head row/.style={before row=\toprule, after row=\midrule},
		every last row/.style={after row=\bottomrule},
		every col no 0/.style={
			string type,
			column name={Inference Method},
			column type={@{}l}},
		% column type={r},
		every col no 1/.style={
			precision=3,
			fixed zerofill=true,
			column type={c}},
		every col no 2/.style={
			column type={c@{}},
			precision=3,
			fixed zerofill=true
		},
	]\transpose
	\caption{p-values of $\chi^2$ test on different models using different inference procedures}
	\label{tab:chi-pvals}
\end{table}
				
Table \ref{tab:chi-pvals} shows the results of carrying out the test on all inference procedures for different discrete models. All of the values are below 0.05, so we reject the null hypothesis in each case. Therefore we conclude that, at the 5\% significance level, the distributions are significantly similar.
				
\subsection{Kolmogorov-Smirnov}
				
The Kolmogorov-Smirnov test is a non parametric test which is used to compare a set of samples with a distribution - this is the one-sample K-S test. There is also a two-sample K-S test, which compares two sets of samples against each other. I use the one-sample test here to compare samples taken from the inferred posteriors to their exact analytic solutions.
				
The test statistic is as follows, with $F_n(x)$ being the empirical cumulative distribution of n samples, and $F(x)$ being the exact cumulative distribution.
\begin{align*}
	F_{n}(x) & =\frac{1}{n}\sum_{i=1}^{n}I_{[-\infty ,x]}(X_{i}) \\
	D_{n}    & =\sup_{x}|F_{n}(x)-F(x)|                          
\end{align*}
This test statistic is compared against the critical values of the Kolmogorov distribution, rejecting the null hypothesis if $\sqrt{n}D_n > K_\alpha$, where $K_\alpha$ is the critical value at the significance level $\alpha$, and $n$ is the number of samples.

\subsubsection{Results}

\begin{table}[!ht]
	\centering
	% \csvautotabular{data/hypothesis/hypothesis-ks.csv}
	\pgfplotstableread[col sep=comma,]{data/hypothesis/hypothesis-ks.csv}\normal
	\pgfplotstabletranspose[colnames from = 0]\transpose\normal
	\pgfplotstabletypeset[
		string type,
		% every col no 1/.style={precision=1}
		% every row/.style={/pgf/number format/sci},
		every head row/.style={before row=\toprule, after row=\midrule},
		every last row/.style={after row=\bottomrule},
		every col no 0/.style={
			string type,
			column name={Inference Method},
			column type={@{}l}},
		every col no 1/.style={
			column type={c@{}},
			precision=4,
			fixed zerofill=true
		},
	]\transpose
	\caption{p-values of K-S test on different models using different inference procedures}
	\label{tab:ks-pvals}
\end{table}
Table \ref{tab:ks-pvals} shows that for the biased coin model, the p-value obtained from all tests are greater than then 0.05. This means we do not reject $H_0$ for any inference procedure, so we can be confident (at the 5\% significance level) that the inference procedures are correct. We can be more sure of some inference procedures, indicated by higher p-values. These results show that there is no significant difference between the generated posterior and the real solution.
		
\FloatBarrier
						
\section{Convergence of sampling}
I also used the KL-divergence metric to determine the (dis)similarity of two distributions. The formula for KL Divergence of discrete distributions $P$ and $Q$ is easy to calculate by \eqref{eq:kl_disc}
% 
\begin{equation} \label{eq:kl_disc}
	{D_{\text{KL}}(P\parallel Q)=\sum _{x\in {\mathcal {X}}}P(x)\log \left({\frac {P(x)}{Q(x)}}\right)} 
\end{equation} 
% 
The continuous version is similar, with $p$ and $q$ now being density functions as in \eqref{eq:kl_cont}. 
% 
\begin{equation} \label{eq:kl_cont}	
	{D_\text{KL}}(P\parallel Q)=\int _{-\infty }^{\infty }p(x)\log \left({\frac {p(x)}{q(x)}}\right)\,dx
\end{equation} 
% 
% Since we cannot compute this integral exactly (we only have the exact density function for one of the distributions), I put the set of samples into discrete bins to approximate $q(x)$. I then used Monte Carlo integration to compute the integral. 
Since I only have a set of samples from $q$, this integral can't be calculated exactly - there is no exact density function. However, there are ways to estimate density functions, as in (Perez 2008) \cite{perez2008kullback}. The first step is to approximate the cdf by finding the empirical cdf, $P_e$ \eqref{eq:pe} then linearly interpolating between points to produce a continuous function $P_c$ \eqref{eq:pc}. Estimating the derivative of $P_c$ then gives the pdf, $\hat{P}$ \eqref{eq:phat}. 
% 
\begin{align}
	P_e(x)     & = \frac1{n}\sum_{i=1}^n U(x-x_i) ,        & \text{ where $U$ is the step function}\label{eq:pe}    \\ 
	P_c(x)     & = \begin{cases}                                                                                
	0          & x<x_0                                                                                          \\
	a_ix+b_i   & x_{i-1} \leq x \le x_i                                                                         \\
	1          & x_{n+1} \leq x                                                                                 
	\end{cases} \label{eq:pc} ,& \text{$a_i$ and $b_i$ chosen to make $P_c$ continuous }\\ 
	\hat{P}(x) & = \frac{P_c(x+\delta) - P_c(x)}{\delta} , & \text{ for sufficiently small }\delta  \label{eq:phat} 
\end{align}
% 
The final estimator \eqref{eq:final_kl_est} is given by using the exact pdf of $q$ and performing Monte Carlo integration.
% 
\begin{equation}
	\label{eq:final_kl_est}
	\hat{D}_{KL}(P \| Q) = \frac1{n}\sum_{i=1}^n \log\left(\frac{\hat{P}(x_i)}{q(x_i)}\right) 
\end{equation}
% 
Both the metrics (discrete and continuous) were computed with code written using the \texttt{EmpiricalDist} modules.
						
The idea behind conducting this test is ensuring that the KL divergence decreases as we take more samples from the posterior. This ensures that the solution converges to the correct distribution - a KL divergence of 0 implies the distributions are identical. 
						
\begin{figure}[H]
	\centering
	\input{tikz/kl_plot.tex}
	\caption{Plot of KL-divergence with increasing number of samples for different models and inference procedures - average over 20 runs}
	\label{fig:kl}
\end{figure}
						
Figure \ref{fig:kl} shows the results of this test. For each inference procedure, we can see that the KL-divergence for each model generally decreases as we take more samples. Rejection sampling is consistently the worst performing inference procedure, with particle based methods such as smc or importance sampling generating more accurate distributions. These plots have all been smoothed by taking a moving average in order to reduce the impact of noise.
						
						
						
\section{Performance}
% probabilistic c (Paige14) makes the same comparison
% TODO: measure performance with varying no. particles (measure KL with difference no. particles)
						
I evaluated the performance of OwlPPL against Anglican and WebPPL. All of these languages are universal PPLs embedded in different host languages, so are comparable to my PPL. I only compare the inference algorithms for which there are comparable implementations in the other languages.
						
Figures \ref{fig:time-perf} and \ref{fig:mem-perf} shows how my PPL compares against these languages for a range of models and inference procedures. All the models have been introduced previously, and have been shown to produce correct results in my PPL when using the given inference procedures. I measure both running time\footnote{timed using libraries in respective languages} and peak memory usage\footnote{computed using \texttt{time -f "\%M"}}.
						
\begin{figure}[!ht]
	\centering
	\input{tikz/times_plot.tex}
	\caption{Time taken for inference against other languages for different models and inference algorithms, taking 10,000 samples from the posterior, averaged over 20 runs. Error bars show the 95\% confidence interval}
	\label{fig:time-perf}
\end{figure}
						
\begin{figure}[!ht]
	\centering
	\input{tikz/mems_plot.tex}
	\caption{Memory Usage of my PPL, compared against other languages for different models, all using an MCMC algorithm, taking 10,000 samples from the posterior, averaged over 20 runs. Error bars show the 95\% confidence interval}
	\label{fig:mem-perf}
\end{figure}
						
These graphs show that OwlPPL performs consistently better that WebPPL in both memory and time. OwlPPL slightly outperforms Anglican on the two continuous models, but is slower than Anglican for the discrete models. This may be because Anglican uses a more efficient representation for discrete distributions, and my representation may need to be optimised. Anglican is a Clojure library, which runs on the JVM, whereas WebPPL is run using nodejs, a JavaScript interpreter. It is possible there is an interpretive overhead with webPPL, explaining slower running times. 
						
My PPL is significantly better than both Anglican and WebPPL in terms of peak memory usage. All three languages are garbage collected, but OCaml does not run code through a virtual machine, and native binaries are generated. This could explain the lower memory usage, with the overheads of the JVM and the node runtime dominating in those languages.
						
% SMC bad:
% This is possibly because when using a large number of particles, a large number of small memory allocations are made by OCaml, which introduced overhead both to my program and the garbage collector.  The languages I compare with are both also garbage collected, but may be more optimised for this use case. 
						
% TODO: add clustering model
I also measured the running time of inference with increased data used - I tested the linear regression model, increasing the length of the arrays used as input. Models which are conditioned on more data are expected to give more accurate results, and my results show that running time increases linearly with the size of data. Figure \ref{fig:time-datasize} shows that all the inference functions run in time linear to the size of the input, but the constant factors vary substantially - for example, Sequential Monte Carlo has a much steeper gradient than Metropolis-Hastings, for example, but both have the same shape. This also shows how some inference procedures take much more time than others, however, this often results in more accurate results. For example, smc is slower than rejection sampling here, but Figure \ref{fig:kl} shows that smc produces posterior distributions which are closer to exact solutions. The y axis has a log scale, showing the large difference in running time for each method.
						
\begin{figure}[!ht]
	\centering
	\input{tikz/linreg_by_datasize.tex}
	\caption{Time taken for inference as a function of input data length as the mean of 10 runs each taking 1,000 samples from the posterior, shaded areas are the 95\% confidence interval ($\pm 2\sigma$)}
	\label{fig:time-datasize}
\end{figure}
						
% For sequential Monte Carlo algorithms, I compared running times with regards to the number of particles used.
						
% don't have just one subsection
% do i need to talk about profiling at all - what does it show?
% \subsection{Profiling}
% \subsubsection{Spacetime}
% \subsubsection{GProf}
