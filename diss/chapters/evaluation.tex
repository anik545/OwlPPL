% !TeX root = ../diss.tex


% sketch out what plots to include here
% - plot of kl divergence w.r.t. number of samples taken
% - plot of running time w.r.t. amount of data conditioned on
% - plot of running time w.r.t. number of particles (for smc)
% - maximum memory footprint (against parameters as above)
% - total memory footprint over time
% box plots instead of just lines

% use ppl to do evaluation? good to show code in my language which does evaluation

% explain WHY in evaluation, how just what - e.g. explain why plot looks how it does, don't just describe

% show some output from merlin to show the fact that type inference works and is good?
So far, I have developed a PPL that can be used to define arbitrary probabilistic models and perform Bayesian inference on them. To evaluate the performance of my PPL, I will present some examples to show programs written in my PPL translated into equivalent programs in other PPLs, and then measure time and memory consumption of inference\footnote{All tests are carried out on a single core of an Intel\textsuperscript{(R)} Core\textsuperscript{(TM)} i5-7200U CPU @ 2.50GHz}. I will also determine the correctness of inference procedures by using hypothesis tests which use drawn samples to determine whether two distributions are the same or not. For all these tests, I will use simple models with analytic solutions to compare to.


\section{Statistical tests}
To evaluate the correctness of my PPL, I used statistical tests which measure goodness-of-fit, i.e. how similar two distributions are to each other. I compare the empirical distribution of 10,000 samples from an approximated distribution to an exact distribution which is calculated analytically. Test statistic distributions (e.g. the $\chi^2$ distribution) were calculated using \texttt{Owl}, and empirical distributions generated using the \texttt{EmpiricalDist} modules.

For all tests described below, I set the significance level, $\alpha = 0.05$ and use null and alternative hypotheses as follows:

$H_0:$ The sample data follow the exact distribution\\
$H_1:$ The sample data do not follow the exact distribution

\subsection{Chi-squared}

The $\chi^2$ test is a simple goodness-of-fit test which can test whether or not a given discrete distribution 


The test statistic is as follows, with each $i$ being a distinct element in the distribution, $x_i$ is the observed number of samples with the value $i$, and $m_i$ is the expected number of samples for the value $i$.

$$X^{2}=\sum _{i=1}^{k}{\frac {(x_{i}-m_{i})^{2}}{m_{i}}}$$

This test statistic is compared against the critical value (at the significance level) of the chi-squared distribution, with the degrees of freedom being $k-1$, where k is the number of possible values of the distribution.
\subsubsection{Results}
\begin{table}[!ht]
	\centering
	% \begin{tabular}{|l|l|l|l|l|}
	% 	\hline
	% 	          & rejection & importance & metropolis-hastings & particle filter \\ \hline
	% 	sprinkler &           &            &                     &                 \\ \hline
	% 	dice      &           &            &                     &                 \\ \hline
	% \end{tabular}
	\csvautotabular{data/hypothesis-chi.csv}
	\caption{p-values of $\chi^2$ test on different models using different inference procedures}
	\label{tab:chi-pvals}
\end{table}

Table \ref{tab:chi-pvals} shows the results of carrying out the test on several inference procedures for different discrete models. None of the values are below 0.05, so we cannot reject the null hypothesis, so we conclude that, at the 5\% significance level, the distributions are not significantly different.

\subsection{Kolmogorov-Smirnov}

The Kolmogorov-Smirnov test is a non parametric test which is used to compare a set of samples with a distribution - this is the one-sample K-S test. There is also a two-sample K-S test, which compares two sets of samples against each other. I use the one-sample test here to compare samples taken from the inferred posteriors to their exact analytic solutions.

The test statistic is as follows, with $F_n(x)$ being the empirical cumulative distribution of n samples, and $F(x)$ being the exact cumulative distribution.

$$F_{n}(x)=\frac{1}{n}\sum_{i=1}^{n}I_{[-\infty ,x]}(X_{i})$$\\
$$D_{n}=\sup_{x}|F_{n}(x)-F(x)|$$

This test statistic is compared against the critical values of the Kolmogorov distribution, rejecting the null hypothesis if $\sqrt{n}D_n > K_\alpha$, where $K_\alpha$ is the critical value at the significance level $\alpha$, and $n$ is the number of samples.

\subsubsection{Results}

Table \ref{tab:ks-pvals} shows that for all the continuous models considered, the p-value obtained from all tests are greater than then 0.05. This means we do not reject $H_0$ for any model/inference procedure combination, so can be confident (at the 5\% significance level) that the inference procedures are correct. This shows that the generated posterior is not significantly different from the real solution.

\begin{table}[!ht]
	\centering
	% \begin{tabular}{|l|l|l|l|l|}
	% 	\hline
	% 	            & rejection & importance & metropolis-hastings & particle filter \\ \hline
	% 	single coin &           &            &                     &                 \\ \hline
	% 	hmm         &           &            &                     &                 \\ \hline
	% \end{tabular}
	\csvautotabular{data/hypothesis-ks.csv}
	\caption{p-values of K-S test on different models using different inference procedures}
	\label{tab:ks-pvals}
\end{table}


\section{Convergence of sampling}
% TODO: add a few lines to show difference between 
I also used the KL-divergence metric to determine the (dis)similarity of two distributions. The formula for KL Divergence of discrete distributions $P$ and $Q$ is

$${D_{\text{KL}}(P\parallel Q)=\sum _{x\in {\mathcal {X}}}P(x)\log \left({\frac {P(x)}{Q(x)}}\right)}$$

The continuous version is similar, with $p$ and $q$ now being density functions:

$${D_\text{KL}}(P\parallel Q)=\int _{-\infty }^{\infty }p(x)\log \left({\frac {p(x)}{q(x)}}\right)\,dx$$

Since we cannot compute this integral exactly (we only have the exact density function for one of the distributions), I put the set of samples into discrete bins, and then used the discrete formula. Both the metrics (discrete and continuous) were computed with code written using the \texttt{EmpiricalDist} modules.s

The idea behind conducting this test is ensuring that the KL divergence decreases as we take more samples from the posterior. This ensures that the solution converges to the correct distribution. I also investigate the rate of convergence using this method.

\begin{figure}[!ht]
	\centering
	\begin{tikzpicture}
		\pgfplotstableread[col sep = comma]{data/kl_coin_all.csv}\datatable
		\pgfplotstablegetcolsof{\datatable}
		\pgfmathtruncatemacro\numberofcols{\pgfplotsretval-1}
		\begin{loglogaxis}[
				title={Coin model}, 
				xlabel={number of samples}, 
				ylabel={KL divergence},
				width=0.45\textwidth
			]
			\addplot table [x index=0, y index=1, col sep=comma] {data/coin_mh.csv};
			\addlegendentry{mh}
			% 
			\addplot table [x index=0, y index=1, col sep=comma] {data/coin_imp.csv};
			\addlegendentry{importance}
			% 
			\addplot table [x index=0, y index=1, col sep=comma] {data/coin_rej.csv};
			\addlegendentry{rejection}
			% 
			\addplot table [x index=0, y index=1, col sep=comma] {data/coin_smc.csv};
			\addlegendentry{smc}
			% % 
			% \pgfplotsinvokeforeach{1,...,\numberofcols}{
			% 	\addplot table [x index=0,y index=#1, col sep=comma] {data/kl_coin_all.csv};
			% 	\pgfplotstablegetcolumnnamebyindex{#1}\of{\datatable}\to{\colname}
			% 	\addlegendentryexpanded{\colname}
			% }
		\end{loglogaxis}
	\end{tikzpicture}
	\begin{tikzpicture}
		\begin{loglogaxis}[
				title={Sprinkler model}, 
				xlabel={number of samples}, 
				ylabel={KL divergence},
				width=0.45\textwidth
			]
			\addplot table [x index=0, y index=1, col sep=comma] {data/sprinkler_mh.csv};
			\addlegendentry{mh}
			% 
			\addplot table [x index=0, y index=1, col sep=comma] {data/sprinkler_rej.csv};
			\addlegendentry{rejection}
			% 
		\end{loglogaxis}
	\end{tikzpicture}
	\caption{Plot of KL-divergence with increasing number of samples for different models and inference procedures}
	\label{fig:kl}
\end{figure}


\section{Performance}
% probabilistic c (paige14) makes the same comparison
% TODO: measure performance with varying no. particles (measure KL with difference no. particles)

I evaluated the performance of my ppl against Anglican, WebPPL, and Pyro. All of these languages are universal PPLs embedded in different host languages, so are comparable to my PPL.

Figure \ref{fig:perf} shows how my PPL compares against these languages for a range of models and inference procedures. All the models have been introduced previously, and have been shown to produce correct results in my PPL when using the given inference procedures. I measure both running time and peak memory usage.

\newcommand{\perfgraph}[2]{
	\begin{tikzpicture}
		\pgfplotstableread[col sep = comma]{#1}\datatable
		\pgfplotstablegetcolsof{\datatable}
		\pgfmathtruncatemacro\numberofcols{\pgfplotsretval-1}
		\begin{axis}[
				ylabel={time (ms)},
				ymin=0,
				legend style={at={(0.5,-0.15)},
					anchor=north,legend columns=-1},
				ybar,
				xtick=data,
				xticklabels from table={\datatable}{model},
				title={#2},
				width=0.45\textwidth
			]
			\pgfplotsinvokeforeach{1,...,\numberofcols}{
				\pgfplotstablegetcolumnnamebyindex{##1}\of{\datatable}\to{\colname}
				\addplot table [x expr=\coordindex,y index=##1, col sep=comma] {#1};
				\addlegendentryexpanded{\colname}
			}
		\end{axis}
	\end{tikzpicture}%
}

\begin{figure}[!ht]
	\centering
	% TODO: ADD ERROR BARS for maybe 95% confidence interval - use ppl to generate confidence interval 
	\perfgraph{data/times_mh.csv}{Metropolis-Hastings}
	\perfgraph{data/times_smc.csv}{Particle Filter}
	\perfgraph{data/times_rej.csv}{Rejection}
	\caption{Performance of my ppl against other languages for different models, all using an MCMC algorithm, taking 10,000 samples from the posterior, averaged over 100 runs}
	\label{fig:perf}
\end{figure}

\begin{figure}[!ht]
	\centering
	% TODO: ADD ERROR BARS for maybe 95% confidence interval - use ppl to generate confidence interval 
	\perfgraph{data/times_mh.csv}{Metropolis-Hastings}
	\perfgraph{data/times_smc.csv}{Particle Filter}
	\perfgraph{data/times_rej.csv}{Rejection}
	\caption{Performance of my ppl against other languages for different models, all using an MCMC algorithm, taking 10,000 samples from the posterior, averaged over 100 runs}
	\label{fig:perf}
\end{figure}


% explain why i chose these languages to compare against?
For sequential Monte Carlo algorithms, I compared running times with regards to the number of particles used.
				
% don't have just one subsection
% do i need to talk about profiling at all - what does it show?
% \subsection{Profiling}
% \subsubsection{Spacetime}
% \subsubsection{GProf}

\section{Comparison to other PPLs}

\subsection{A simple model}
\subsection{A more complex model}
