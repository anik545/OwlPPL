% !TeX root = ../diss.tex

In this chapter, I will discuss the research done before starting the project, and some of the design decisions made based on this. In particular, common patterns in OCaml (and functional programming in general), influenced the final DSL, as well as the design of other similar probabilistic programming systems.

\section{Starting Point}

There do exist PPLs for OCaml, such as IBAL \cite{kiselyov2009embedded}, as well as PPLs for other languages, such as WebPPL - JS\cite{mobus2018structure}, Church - LISP\cite{goodman2012church} or Infer.Net - F\#\cite{wang2011using} to name a few. My PPL can draw on some of the ideas introduced by these languages, particularly in implementing efficient inference engines. I will need to research the different approaches taken by these PPLs and decide what form of PPL to implement, especially in deciding the types of model I will want my PPL to be able to represent and the inference methods I implement.

I will be using an existing OCaml numerical computation library (Owl). This library does not contain methods for probabilistic programming in general, although it does contain modules which will help in the implementation of an inference engine such as efficient random number generation and lazy evaluation.

I have experience with the core SML language, which will aid in learning basic OCaml due to similarities in the languages, however I will still have to learn the modules system. 1B Foundations of Data Science also gives me a basic understanding of Bayesian inference. I do not have experience with domain specific languages in OCaml, although the 1B compilers course did implement a compiler and interpreter in OCaml.

\section{Requirements}

Before starting to write any code, I made sure to set out the features I aimed to implement for my DSL. The main goal was to produce a usable language, which was defined by the following criteria:

% TODO: which tense to use here - past, say "this met the success criteria as follows"
\begin{itemize}
    \item \textbf{Language Features}: Since I have written an embedded DSL, a user of my PPL should be able to take advantage of all standard OCaml features in the deterministic parts of their models. I need to make sure that this is the case, and features such as recursive functions will work.
    \item \textbf{Available distributions}: I aimed to make sure my PPL has at minimum the bernoulli and normal distributions available as basic building blocks to build more complex probabilistic programs.
    \item \textbf{Correctness} of inference: I used the PPL developed on example problems to ensure correct results are produced. These results were compared to results produced in other PPLs as well as comparisons to analytic solutions for simple problems.
    \item \textbf{Available Inference Algorithms}: I aimed to include at least one available inference algorithm. However, since different problems are more or less well suited to different general-purpose inference procedures, I wrote implementations for five separate algorithms.
    \item \textbf{Performance}: This is a quantitative measure, comparing programs written in my PPL to equivalent programs in other PPLs. I used the profiling tools spacetime and gprof to profile my OCaml code. Performing inference should be possible within a reasonable amount of time, even though the project does not have a significant focus on performance. I also benchmarked the performance with regards to scalability, i.e. made sure the performance is still reasonable as models are conditioned on more data.
\end{itemize}

\section{Professional Practice}

I adopted several best practices in order to ensure the project was successful. This includes performing regular testing, splitting code into separate modules designing signatures first, and ensuring my code follows a consistent style guide (Jane Street style\footnote{\url{https://opensource.janestreet.com/standards/}})

\subsection{Testing}

Testing systems which are linked to randomness can be quite tricky, as it is difficult to test behaviour that is expected to change from one execution to the next. One approach is to set a fixed random seed and make sure the same sequence of results are produced. The aim of a unit test, however, is to make sure that a desired property does not change from one version of the code to the next. Even with a fixed random seed, a change in code may cause new outputs even though the fundamental statistical property desired hasn't changed. Another approach is to perform some statistical test such as kolmogorov-smirnov \cite{}, to ensure distributions produced by my library are equal to what is expected. A problem with tests of this kind is that they are expected to fail sometimes, meaning unit tests will provide limited utility due to their inherent flakiness with random programs. In fact, since we expect these tests to fail a certain percentage of the time, if they do not fail they show a problem with our program.

As such, most of the unit tests I wrote are fairly simple and only catch very basic bugs. Unit tests were possible to write for the exact inference procedure when working with discrete distributions, since we always expect the same output distribution. However, all the other inference algorithms are approximate, so tests suffer from the problems described above.

Despite this, I will still be able to carry out a full evaluation, performing hypothesis tests such as the kolmogorov-smirnov or chi-squared tests.

% talk about using very basic unit tests
% unit testing ppl is 
% include maths about hypothesis tests here - HAVE TO FAIL SOMETIMES otherwise the program is wrong
% include unit tests for exact inference
% go though how to solve this in implementations
% find some sources for how other people solve this, reference that here.

\subsection{Licenses}

The major libraries I use, Owl and Jane Street Core, are both licensed under the MIT license.
% what else do i write here - the pink book says to talk about licenses

\section{Tools and Technologies}
The main tools I used are listed here:
\begin{itemize}
    \item Ocaml 4.08 - The language I wrote the main PPL library in
    \item Dune - Build system for OCaml
    \item Opam - OCaml package manager
    \item Alcotest - Unit testing framework
    \item Quickcheck - Random testing library
    \item Spacetime - Memory profiler for OCaml
    \item Landmarks - Profiler for OCaml
    \item Owl - Scientific computing library in OCaml
    \item VSCode (with ocaml extensions) - IDE for OCaml development
    \item Git with GitHub - version control
\end{itemize}

Using OCaml 4.08 allows me to use new features of OCaml, in particular the ability to define custom let operators as syntax sugar for monads. The dune build system also allows me to more easily manage building and testing my code, as well as automatically creating documentation from comments and function signatures in my code. The profilers I used allowed me to work out the causes of performance issues and remedy them. 

\subsection{Continuous Integration}
Since I will be developing a library, it is important to make sure that any unit tests are run regularly to ensure there are no regressions - no new code affects old behaviour. It is also important to make sure the library will function on different platforms, and that documentation is built (and possibly uploaded) automatically. To achieve this, I will be using GitHub's continuous integration service, `actions' to make sure the code on the master branch always works. The CI can also be used to ensure the library works with older versions of OCaml, and is backwards-compatible.

% This feels like it should be in implementation.
\section{Language Design}
I chose to implement my language as a domain specific language (DSL) shallowly embedded into the main OCaml language. This allows models built in the ppl to be easily composed with other standard OCaml programs.

Using a shallow embedding means we can use all of the features of OCaml as normal, including branching (if/then/else), loops, references, functions, and importantly, recursion. This can allow us to define models that do not terminate and are therefore invalid. However, we can write functions which are \textit{stochastic recursive}, that is, functions which have a probability of termination that tends to 1 as the number of successive calls tends to infinity. This leads to functions which terminate their recursion non-deterministically.

I use a set of primitive distributions which can be combined (using arbitrarily complex deterministic OCaml code) to produce new more complex distribution. For example, one can take the sum of two discrete uniform distributions to simulate the addition of two dice rolls. 

PPLs in general are similar to normal programming languages, but need to support two extra operations - \texttt{sample} and \texttt{condition} \cite{}. The sample function is for sampling from some distribution - either a primitive distribution or a user defined distribution.

\section{OCaml}
I have chosen to use the OCaml language to implement my PPL. There are many features of OCaml which make it suitable for writing a DSL. Using OCaml, I can make sure that expressions in the DSL are well-typed, so that programs don't fail to run. OCaml's algebraic datatypes also make it easy to represent probabilistic programs as trees, and pattern matching makes it easy to 'interpret' and transform these trees. The module system also makes sure that certain types are hidden from the user, which can ensure only valid structures are created by the user.

\section{Owl}

Owl is a scientific computing library written for OCaml \cite{owl}. Importantly for my PPL, it contains functions for working with multi-dimensional arrays, as well as a wide variety of statistical functions. In particular, it contains functionality relating to many common distributions, efs.g. normal, beta, binomial, etc. Since my language will allow the user to combine these basic distributions into larger models, I will need to use these functions to allow sampling from and the ability to perform inference on these models. In particular, it is important to be able to find the probability density function (pdf) and cumulative density function (cdf) of these distributions as well as sample from them. Another important feature of owl is it's plotting functionality which enabled me to write functions which let me visualise output distributions, as well as visualising performance statistics, all directly from OCaml.

% Should these parts be here? Do I explain the methods here or in the implementation section?
\section{Probability Monad}
% how in-depth should my explanation of a monad be?

\subsection{Monads}

Monads are a design pattern commonly used in functional programming languages.

The key data structure I use to model probability distributions is a monad. This allows me to compose distributions easily, by using the output from one distribution in another using the bind operator. In OCaml, I can also use the new (let*) operators to make this more ergonomic, and make code look more natural (as opposed to using the \texttt{>>=} operator everywhere).

\section{Approaches to probabilistic programming}
Existing PPLs take several different forms, both as standalone and embedded languages. The main tradeoff that is made in the design of PPLs is the number of models that can be expressed in the language compared to how efficient inference is. One approach is graph-based, where a \textit{factor graph} is generated from the program, over which efficient inference can take place. This approach can be seen in languages such as infer.NET or JAGS, and has the benefit of very fast inference, particularly since efficient computation graph frameworks can be leveraged - an example is edward \cite{}, which uses tensorflow. However, these languages usually cannot represent infinite models or unbounded recursion. Another approach, taken by the likes of webppl or anglican, is trace-based. This approach considers execution traces, with a `trace' being one run of a program, with all the intermediate variables taking a particular value. Inference algorithms can reason about these traces in order to produce a posterior distribution, as will be seen in the next section. A trace-based approach often leads to clearer programs, since we are not working with a computation graph. It also leads to more models being able to be expressed, since we are not limited by the constraints of a graph. However inference is often slower, particularly when dealing with data in high dimensions, since the algorithms need to be more general purpose, and often converge slower.

\section{Bayesian Inference}
Inference is the key motivating feature of probabilistic programming, and is a way to infer a distribution over the parameters based on the data we observe. The main feature of bayesian inference is that we assign every model some prior belief. Often this prior is chosen based on our knowledge of the problem, but the prior can also be uninformative. The goal of bayesian inference is to calculate the posterior distribution, which can be represented by bayes formula,


$$P(A\mid B)={\frac {P(B\mid A)P(A)}{P(B)}}$$

with $P(A)$ being the prior, and $P(B\mid A)$ being the likelihood model we define (which is a program in the PPL setting).

This formula hold for the continuous case too, using probability density functions ($f_X$),

$$ f_{X\mid Y=y}(x)={\frac {f_{Y\mid X=x}(y)f_{X}(x)}{f_{Y}(y)}} $$

Unfortunately, exact bayesian inference is usually computationally infeasible, especially when the number of random variables we consider is large. For example, consider a set of discrete random variables ${X_i}$, with p the joint probability distribution of all $X_i$. The marginal distribution of any individual $X_i$ is:

$$P(X_i=x_i) = \Sigma_{} $$

If we have 50 variables which can take one of two values, then we have to sum over $2^{50}$ values. There do exist some algorithms which operate on bayesian networks (DAGs which represent random variables and their interdependence), and reduce the number of calculations needed. These include methods such as \textit{Variable Elimination} or \textit{Message Passing}.

Alternatively, in the continuous case the formula 

$$P(\theta|x)=P(x|\theta)P(\theta)\over{P(x)}$$
with 
$$ P(x)=\int_{\Theta}P(x,\theta)d\theta $$

The normalizing constant here is an integral that often does not have an analytic solution, and so must be approximated. Another issue is that directly sampling from a distribution requires that we also invert this formula.

\section{Inference Algorithms}
% https://ermongroup.github.io/cs228-notes/inference/sampling/
% https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture17.pdf

% https://www.cs.ubc.ca/~fwood/teaching/OXWASP_CDT/probabilistic_programming.pdf
% http://www.robots.ox.ac.uk/~fwood/anglican/assets/pdf/Wood-AISTATS-2014.pdf
Inference algorithms are ways to systematically generate samples from posterior distributions given a generative model and a prior. In probabilistic programming, a model consists of latent variables and observed variables, and a single execution of a model (a program) can be thought of as an assignment to each of these variables, known as an \textit{execution trace}. This can be defined mathematically as below, by bayes rule:

$$p(x_{1:N}|y_{1:N})\propto Ìƒ\tilde{p}(y_{1:N},x_{1:N})$$ 

Here, $p$ is the posterior distribution of a particular trace $x$, given the observed variables $y$
The aim then is to find the posterior over the latent variables we are interested in, which we can specify within the program, either as part of the model, or outside it in a query to the model.

Show how this relates to a program here or in implementation?

\subsection{Exact Inference}

Exact Inference is the simplest method of calculating the posterior, but is usually computationally intractable. It involves calculating bayes formula exactly, of which calculating the normalising constant is usually the problem. To sample directly, we would also need to find the inverse cumulative distribution to be able to use the inversion sampling method.

For discrete posterior distributions it can be thought of as calculating the probability of every possible value of the variable of interest. Since a random variable will be dependent on several others, this involves finding every possible combination of these variables and their outcomes.

\subsection{Rejection Sampling}

Since exact inference is too difficult in practice, we usually have to resort to \textit{Monte Carlo} sampling methods.

One such method, rejection sampling, is a very simple inference method which takes uses another distribution which can be sampled from.

\subsection{Importance Sampling}

Importance sampling is another simple method, improving on rejection sampling, that can be used to sample from a target distribution using another distribution, known as the proposal distribution. We then calculate the ratio of the likelihoods between the two distributions to weight samples from the proposal. From doing this repeatedly with multiple samples from the proposal, we can build a posterior represented by a set of weighted samples.

% put proofs of inference methods here/implementation/appendix?
% Add equations

\subsection{Monte Carlo Markov Chains (MCMC)}

MCMC methods involve constructing a Markov chain with a stationary distribution equal to the posterior distribution. A markov chain is a statistical model consisting of a sequence of events, where the probability of any event depends only on the previous event. The stationary distribution is the distribution over successive states that the chain converges to.

There exists several algorithms for finding this markov chain, for example metropolis-hastings. This algorithm requires that we have a function, $f(x)$, which is proportional to the density of the distribution. This function is easy to compute for the posterior, since it is simply the prior multiplied by the posterior - the normalising constant can be ignored since we only need a proportional function.

\subsection{Sequential Monte Carlo (SMC)}

SMC methods are algorithms which are based on using large numbers of weighted samples (`particles') to represent a posterior distribution. These particles are updated and resampled from in order to converge the set of particles to the posterior.

